{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тема 2. “Создание признакового пространства”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Создайте мешок слов с помощью sklearn.feature_extraction.text.CountVectorizer.fit_transform(). Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно.\n",
    "\n",
    "●\tИгнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью max_df.\n",
    "\n",
    "●\tОграничим количество слов, попадающий в мешок, с помощью max_features = 1000.\n",
    "\n",
    "●\tИсключим стоп-слова с помощью stop_words='english'.\n",
    "\n",
    "●\tОтобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью CountVectorizer.get_feature_names().\n",
    "\n",
    "#### 2. Создайте мешок слов с помощью sklearn.feature_extraction.text.TfidfVectorizer.fit_transform(). Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно.\n",
    "\n",
    "●\tИгнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью max_df.\n",
    "\n",
    "●\tОграничим количество слов, попадающий в мешок, с помощью max_features = 1000.\n",
    "\n",
    "●\tИсключим стоп-слова с помощью stop_words='english'.\n",
    "\n",
    "●\tОтобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью TfidfVectorizer.get_feature_names().\n",
    "\n",
    "#### 3. Натренируем gensim.models.Word2Vec модель на наших данных.\n",
    "\n",
    "●\tТренировать будем на токенизированных твитах combine_df['tweet_token']\n",
    "\n",
    "●\tУстановим следующие параметры: size=200, window=5, min_count=2, sg = 1, hs = 0, negative = 10, workers= 32, seed = 34.\n",
    "\n",
    "●\tИспользуем функцию train() с параметром total_examples равным длине combine_df['tweet_token'], количество epochs установим 20.\n",
    "\n",
    "#### 4. Давайте немного потестируем нашу модель Word2Vec и посмотрим, как она работает. Мы зададим слово positive = \"dinner\", и модель вытащит из корпуса наиболее похожие слова c помощью функции most_similar. То же самое попробуем со словом \"trump\".\n",
    "\n",
    "#### 5. Из приведенных выше примеров мы видим, что наша модель word2vec хорошо справляется с поиском наиболее похожих слов для данного слова. Но как она это делает? Она изучила векторы для каждого уникального слова наших данных и использует косинусное сходство, чтобы найти наиболее похожие векторы (слова).\n",
    "Давайте проверим векторное представление любого слова из нашего корпуса, например \"food\".\n",
    "\n",
    "#### 6. Поскольку наши данные содержат твиты, а не только слова, нам придется придумать способ использовать векторы слов из модели word2vec для создания векторного представления всего твита. Существует простое решение этой проблемы, мы можем просто взять среднее значение всех векторов слов, присутствующих в твите. Длина результирующего вектора будет одинаковой, то есть 200. Мы повторим тот же процесс для всех твитов в наших данных и получим их векторы. Теперь у нас есть 200 функций word2vec для наших данных.\n",
    "\n",
    "Необходимо создать вектор для каждого твита, взяв среднее значение векторов слов, присутствующих в твите. В цикле сделать:  vec += model_w2v[word].reshape((1, size))\n",
    "и поделить финальный вектор на количество слов в твите.\n",
    "На выходе должен получиться wordvec_df.shape = (49159, 200).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1\n",
    "Создайте мешок слов с помощью sklearn.feature_extraction.text.CountVectorizer.fit_transform(). Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../01/tweets.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>[when, father, is, dysfunctional, and, is, so,...</td>\n",
       "      <td>[father, dysfunct, selfish, drag, kid, dysfunc...</td>\n",
       "      <td>[father, dysfunctional, selfish, drag, kid, dy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>[thanks, for, lyft, credit, can, not, use, cau...</td>\n",
       "      <td>[thank, lyft, credit, use, caus, offer, wheelc...</td>\n",
       "      <td>[thank, lyft, credit, use, cause, offer, wheel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1    0.0   @user when a father is dysfunctional and is s...   \n",
       "1   2    0.0  @user @user thanks for #lyft credit i can't us...   \n",
       "\n",
       "                                         tweet_token  \\\n",
       "0  [when, father, is, dysfunctional, and, is, so,...   \n",
       "1  [thanks, for, lyft, credit, can, not, use, cau...   \n",
       "\n",
       "                                       tweet_stemmed  \\\n",
       "0  [father, dysfunct, selfish, drag, kid, dysfunc...   \n",
       "1  [thank, lyft, credit, use, caus, offer, wheelc...   \n",
       "\n",
       "                                    tweet_lemmatized  \n",
       "0  [father, dysfunctional, selfish, drag, kid, dy...  \n",
       "1  [thank, lyft, credit, use, cause, offer, wheel...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['father', 'dysfunct', 'selfish', 'drag', 'kid', 'dysfunct', 'run'],\n",
       " ['thank',\n",
       "  'lyft',\n",
       "  'credit',\n",
       "  'use',\n",
       "  'caus',\n",
       "  'offer',\n",
       "  'wheelchair',\n",
       "  'van',\n",
       "  'pdx',\n",
       "  'disapoint',\n",
       "  'getthank'],\n",
       " ['bihday', 'majesti'],\n",
       " ['model', 'love', 'take', 'time', 'ur']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = df['tweet_stemmed']\n",
    "texts = texts.to_list()\n",
    "texts[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# приводим тексты твитов в формат, который принимает векторайзер (список предложений)\n",
    "all_text_stemmed = []\n",
    "for row in texts:\n",
    "    row_text = \"\"\n",
    "    for word in row:\n",
    "        row_text = row_text + \" \" + word\n",
    "    all_text_stemmed.append(row_text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['father dysfunct selfish drag kid dysfunct run',\n",
       " 'thank lyft credit use caus offer wheelchair van pdx disapoint getthank',\n",
       " 'bihday majesti',\n",
       " 'model love take time ur',\n",
       " 'factsguid societi motiv',\n",
       " 'huge fan fare big talk leav chao pay disput get allshowandnogo',\n",
       " 'camp tomorrow danni',\n",
       " 'next school year year exam think school exam hate imagin actorslif revolutionschool girl',\n",
       " 'love land allin cav champion cleveland clevelandcavali',\n",
       " 'welcom gr']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# что у нас получилось после преобразования\n",
    "all_text_stemmed[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаём объект CountVectorizer и устанавливаем его параметры\n",
    "# Игнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью max_df.\n",
    "# Ограничим количество слов, попадающий в мешок, с помощью max_features = 1000.\n",
    "# Исключим стоп-слова с помощью stop_words='english'.\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 1), analyzer='word', \n",
    "                                   binary=False, max_df = 0.9, max_features = 1000,\n",
    "                                  stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем the Bag-of-Words модель\n",
    "bag_of_words = count_vectorizer.fit_transform(all_text_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отобразим Bag-of-Words модель как DataFrame\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "bow_df = pd.DataFrame(bag_of_words.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abl</th>\n",
       "      <th>absolut</th>\n",
       "      <th>accept</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actual</th>\n",
       "      <th>ad</th>\n",
       "      <th>adapt</th>\n",
       "      <th>...</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youtub</th>\n",
       "      <th>yr</th>\n",
       "      <th>yummi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abl  absolut  accept  account  act  action  actor  actual  ad  adapt  ...  \\\n",
       "0    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "1    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "\n",
       "   yeah  year  yesterday  yo  yoga  york  young  youtub  yr  yummi  \n",
       "0     0     0          0   0     0     0      0       0   0      0  \n",
       "1     0     0          0   0     0     0      0       0   0      0  \n",
       "\n",
       "[2 rows x 1000 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверяем наш датафрейм, созданный на BoW\n",
    "bow_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Проделаем то же самое для лемм из столбца \"tweet_lemmatized\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['father', 'dysfunctional', 'selfish', 'drag', 'kid', 'dysfunction', 'run'],\n",
       " ['thank',\n",
       "  'lyft',\n",
       "  'credit',\n",
       "  'use',\n",
       "  'cause',\n",
       "  'offer',\n",
       "  'wheelchair',\n",
       "  'vans',\n",
       "  'pdx',\n",
       "  'disapointed',\n",
       "  'getthanked'],\n",
       " ['bihday', 'majesty'],\n",
       " ['model', 'love', 'take', 'time', 'ur']]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# достаём все тексты из поля tweet_lemmatized\n",
    "texts = df['tweet_lemmatized']\n",
    "texts = texts.to_list()\n",
    "texts[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# приводим тексты твитов в формат, который принимает векторайзер (список предложений)\n",
    "all_text_lemmed = []\n",
    "for row in texts:\n",
    "    row_text = \"\"\n",
    "    for word in row:\n",
    "        row_text = row_text + \" \" + word\n",
    "    all_text_lemmed.append(row_text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['father dysfunctional selfish drag kid dysfunction run',\n",
       " 'thank lyft credit use cause offer wheelchair vans pdx disapointed getthanked',\n",
       " 'bihday majesty',\n",
       " 'model love take time ur',\n",
       " 'factsguide society motivation',\n",
       " 'huge fan fare big talk leave chaos pay dispute get allshowandnogo',\n",
       " 'camp tomorrow danny',\n",
       " 'next school year year exams think school exams hate imagine actorslife revolutionschool girl',\n",
       " 'love land allin cavs champion cleveland clevelandcavaliers',\n",
       " 'welcome gr']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# что у нас получилось после преобразования\n",
    "all_text_lemmed[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем the Bag-of-Words модель\n",
    "bag_of_words = count_vectorizer.fit_transform(all_text_lemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отобразим Bag-of-Words модель как DataFrame\n",
    "feature_names2 = count_vectorizer.get_feature_names()\n",
    "bow_df2 = pd.DataFrame(bag_of_words.toarray(), columns = feature_names2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>accept</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actually</th>\n",
       "      <th>adapt</th>\n",
       "      <th>add</th>\n",
       "      <th>...</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youth</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yr</th>\n",
       "      <th>yrs</th>\n",
       "      <th>yummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   able  absolutely  accept  account  act  action  actor  actually  adapt  \\\n",
       "0     0           0       0        0    0       0      0         0      0   \n",
       "1     0           0       0        0    0       0      0         0      0   \n",
       "\n",
       "   add  ...  yesterday  yo  yoga  york  young  youth  youtube  yr  yrs  yummy  \n",
       "0    0  ...          0   0     0     0      0      0        0   0    0      0  \n",
       "1    0  ...          0   0     0     0      0      0        0   0    0      0  \n",
       "\n",
       "[2 rows x 1000 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_df2.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2\n",
    "Создайте мешок слов с помощью sklearn.feature_extraction.text.TfidfVectorizer.fit_transform(). Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаём объект TfidfVectorizer и устанавливаем его параметры\n",
    "# Игнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью max_df.\n",
    "# Ограничим количество слов, попадающий в мешок, с помощью max_features = 1000.\n",
    "# Исключим стоп-слова с помощью stop_words='english'.\n",
    "# Отобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью TfidfVectorizer.get_feature_names().\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1), analyzer='word', \n",
    "                                   binary=False, max_df = 0.9, max_features = 1000,\n",
    "                                  stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем the Bag-of-Words модели для стеммированных текстов и для лемматизированных\n",
    "bag_of_words_lemmed = tfidf_vectorizer.fit_transform(all_text_lemmed)\n",
    "feature_names_lem = tfidf_vectorizer.get_feature_names()\n",
    "bag_of_words_stemmed = tfidf_vectorizer.fit_transform(all_text_stemmed)\n",
    "feature_names_stem = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отобразим Bag-of-Words модель как DataFrame\n",
    "tfidf_df = pd.DataFrame(bag_of_words_lemmed.toarray(), columns = feature_names_lem)\n",
    "tfidf_df2 = pd.DataFrame(bag_of_words_stemmed.toarray(), columns = feature_names_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>accept</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actually</th>\n",
       "      <th>adapt</th>\n",
       "      <th>add</th>\n",
       "      <th>...</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youth</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yr</th>\n",
       "      <th>yrs</th>\n",
       "      <th>yummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.345336</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    able  absolutely  accept  account  act    action  actor  actually  adapt  \\\n",
       "28   0.0         0.0     0.0      0.0  0.0  0.000000    0.0       0.0    0.0   \n",
       "29   0.0         0.0     0.0      0.0  0.0  0.345336    0.0       0.0    0.0   \n",
       "30   0.0         0.0     0.0      0.0  0.0  0.000000    0.0       0.0    0.0   \n",
       "\n",
       "    add  ...  yesterday   yo  yoga  york  young  youth  youtube   yr  yrs  \\\n",
       "28  0.0  ...        0.0  0.0   0.0   0.0    0.0    0.0      0.0  0.0  0.0   \n",
       "29  0.0  ...        0.0  0.0   0.0   0.0    0.0    0.0      0.0  0.0  0.0   \n",
       "30  0.0  ...        0.0  0.0   0.0   0.0    0.0    0.0      0.0  0.0  0.0   \n",
       "\n",
       "    yummy  \n",
       "28    0.0  \n",
       "29    0.0  \n",
       "30    0.0  \n",
       "\n",
       "[3 rows x 1000 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.iloc[28:31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abl</th>\n",
       "      <th>absolut</th>\n",
       "      <th>accept</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actual</th>\n",
       "      <th>ad</th>\n",
       "      <th>adapt</th>\n",
       "      <th>...</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youtub</th>\n",
       "      <th>yr</th>\n",
       "      <th>yummi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.334456</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    abl  absolut  accept  account  act    action  actor  actual   ad  adapt  \\\n",
       "28  0.0      0.0     0.0      0.0  0.0  0.000000    0.0     0.0  0.0    0.0   \n",
       "29  0.0      0.0     0.0      0.0  0.0  0.334456    0.0     0.0  0.0    0.0   \n",
       "30  0.0      0.0     0.0      0.0  0.0  0.000000    0.0     0.0  0.0    0.0   \n",
       "\n",
       "    ...  yeah  year  yesterday   yo  yoga  york  young  youtub   yr  yummi  \n",
       "28  ...   0.0   0.0        0.0  0.0   0.0   0.0    0.0     0.0  0.0    0.0  \n",
       "29  ...   0.0   0.0        0.0  0.0   0.0   0.0    0.0     0.0  0.0    0.0  \n",
       "30  ...   0.0   0.0        0.0  0.0   0.0   0.0    0.0     0.0  0.0    0.0  \n",
       "\n",
       "[3 rows x 1000 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df2.iloc[28:31]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3 Натренируем gensim.models.Word2Vec модель на наших данных.\n",
    "\n",
    "* Тренировать будем на токенизированных твитах combine_df['tweet_token']\n",
    "* Установим следующие параметры: size=200, window=5, min_count=2, sg = 1, hs = 0, negative = 10, workers= 32, seed = 34.\n",
    "* Используем функцию train() с параметром total_examples равным длине combine_df['tweet_token'], количество epochs установим 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['tweet_token'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['when',\n",
       "  'father',\n",
       "  'is',\n",
       "  'dysfunctional',\n",
       "  'and',\n",
       "  'is',\n",
       "  'so',\n",
       "  'selfish',\n",
       "  'he',\n",
       "  'drags',\n",
       "  'his',\n",
       "  'kids',\n",
       "  'into',\n",
       "  'his',\n",
       "  'dysfunction',\n",
       "  'run'],\n",
       " ['thanks',\n",
       "  'for',\n",
       "  'lyft',\n",
       "  'credit',\n",
       "  'can',\n",
       "  'not',\n",
       "  'use',\n",
       "  'cause',\n",
       "  'they',\n",
       "  'do',\n",
       "  'not',\n",
       "  'offer',\n",
       "  'wheelchair',\n",
       "  'vans',\n",
       "  'in',\n",
       "  'pdx',\n",
       "  'disapointed',\n",
       "  'getthanked'],\n",
       " ['bihday', 'your', 'majesty'],\n",
       " ['model',\n",
       "  'love',\n",
       "  'you',\n",
       "  'take',\n",
       "  'with',\n",
       "  'you',\n",
       "  'all',\n",
       "  'the',\n",
       "  'time',\n",
       "  'in',\n",
       "  'ur'],\n",
       " ['factsguide', 'society', 'now', 'motivation']]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=texts, size=200, window=5, min_count=2, \n",
    "                 sg = 1, hs = 0, negative = 10, workers= 32, seed = 34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49159"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9128456, 11676740)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(texts, total_examples=len(texts), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4 \n",
    "Давайте немного потестируем нашу модель Word2Vec и посмотрим, как она работает. Мы зададим слово positive = \"dinner\", и модель вытащит из корпуса наиболее похожие слова c помощью функции most_similar. То же самое попробуем со словом \"trump\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('donald', 0.5501392483711243),\n",
       " ('crony', 0.5327620506286621),\n",
       " ('unfit', 0.5262327194213867),\n",
       " ('suppoer', 0.5197625756263733),\n",
       " ('donaldtrump', 0.5164532661437988),\n",
       " ('dumptrump', 0.5098274350166321),\n",
       " ('impeachment', 0.500749945640564),\n",
       " ('conman', 0.5001658201217651),\n",
       " ('unfavorability', 0.49976640939712524),\n",
       " ('phony', 0.49667587876319885)]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"trump\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bihdaydinner', 0.5735843181610107),\n",
       " ('bolognese', 0.5555020570755005),\n",
       " ('cookout', 0.5464305877685547),\n",
       " ('shawarma', 0.5434701442718506),\n",
       " ('sissy', 0.5316126346588135),\n",
       " ('waterloo', 0.5298169851303101),\n",
       " ('spaghetti', 0.527080237865448),\n",
       " ('burritos', 0.5263620615005493),\n",
       " ('tacotuesday', 0.5226019620895386),\n",
       " ('lastnight', 0.5163992047309875)]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"dinner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'disapointed' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-159-be6c32e17c84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"disapointed\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'disapointed' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"disapointed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 5\n",
    "Из приведенных выше примеров мы видим, что наша модель word2vec хорошо справляется с поиском наиболее похожих слов для данного слова. Но как она это делает? Она изучила векторы для каждого уникального слова наших данных и использует косинусное сходство, чтобы найти наиболее похожие векторы (слова).\n",
    "Давайте проверим векторное представление любого слова из нашего корпуса, например \"food\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.0487982e-01,  7.1753567e-01,  1.6437873e-01, -4.9533224e-04,\n",
       "       -4.0243381e-01,  2.4130361e-01, -1.7375448e-01,  1.9668108e-01,\n",
       "        5.1833510e-01,  7.2512947e-02, -3.7711841e-01, -1.3450913e-01,\n",
       "       -6.1501622e-01,  4.2265847e-02,  1.0030919e+00, -2.6114342e-01,\n",
       "        2.4443835e-02,  5.6554186e-01,  3.3014292e-01,  3.7453169e-01,\n",
       "        2.4988325e-01, -8.7458968e-02,  6.3764280e-01, -4.2551327e-01,\n",
       "        7.8718758e-01, -3.1817943e-01,  1.2278194e-01, -1.0029565e+00,\n",
       "        3.8749295e-01, -2.3035859e-01, -4.2597029e-01, -3.7988597e-01,\n",
       "        4.9015749e-01, -6.0321999e-01, -3.7664428e-01,  3.8577858e-01,\n",
       "       -1.6822539e-01,  5.6258911e-01, -4.2373922e-01, -1.2999626e-01,\n",
       "       -2.8590664e-01,  5.5313420e-01, -2.2693746e-01, -1.3608521e-01,\n",
       "       -6.4299113e-01,  2.0960350e-01, -1.3274111e-01, -4.9843445e-01,\n",
       "       -1.3675939e-01,  2.6674110e-01,  4.2237278e-02,  4.3394662e-02,\n",
       "       -5.5560046e-01, -1.2632187e-01, -4.4564524e-01, -4.6242688e-02,\n",
       "       -2.6184759e-01, -6.8852937e-01, -5.2665526e-01,  7.7356774e-01,\n",
       "       -3.3483304e-02,  1.8740873e-01, -1.4263585e-01, -6.3960135e-01,\n",
       "        3.9438310e-01, -3.8043779e-01,  7.5049222e-01, -2.4091607e-02,\n",
       "        8.3235544e-01, -2.5674272e-01, -2.5011212e-01,  5.0268576e-02,\n",
       "        4.7346011e-02,  2.3800444e-02, -3.4015945e-01, -3.5760015e-01,\n",
       "        8.0075061e-01,  5.9596992e-01, -3.4883174e-01, -1.5500300e-01,\n",
       "       -2.6570040e-01,  2.3179603e-01,  5.2945060e-01,  2.7603361e-01,\n",
       "        3.3864799e-01, -4.3362338e-02,  1.2285306e-01,  2.1278363e-01,\n",
       "        2.9150560e-01, -7.3556408e-02, -4.8083112e-02,  2.7180913e-01,\n",
       "        1.8166043e-01, -4.4898430e-01,  1.2330796e-02, -2.1456532e-01,\n",
       "       -6.2308180e-01,  5.4547995e-01, -1.0433824e+00,  5.3930181e-01,\n",
       "        3.6132309e-01,  5.7421857e-01, -3.0320349e-01, -3.5523310e-01,\n",
       "       -4.7917682e-01,  1.9258200e-01,  2.9334331e-01,  1.3972831e-01,\n",
       "        3.7344680e-03,  4.1523373e-01, -3.7011206e-01,  7.1002573e-01,\n",
       "       -4.7141859e-01,  5.1555139e-01, -2.9442100e-02, -3.7300938e-01,\n",
       "        1.6827925e-01, -7.6328069e-02,  2.6185505e-02,  4.2196313e-01,\n",
       "       -4.6589679e-01, -2.6997393e-01, -4.6027943e-01, -8.4668338e-01,\n",
       "        1.8270352e-01,  9.3102634e-02, -5.5861503e-02, -2.5281957e-01,\n",
       "        2.1937101e-01, -5.6224519e-01,  9.4773926e-02, -1.0958688e-01,\n",
       "       -2.8214762e-01, -2.0505944e-01,  6.4437225e-02,  5.5346394e-01,\n",
       "       -8.0563314e-02, -2.7184871e-01,  5.0157082e-01,  4.4384193e-01,\n",
       "       -4.6333316e-01, -3.9234310e-01, -5.8771305e-02, -7.1571881e-01,\n",
       "        7.4457191e-02, -4.1703045e-02,  2.1811960e-02,  1.2195249e-01,\n",
       "       -9.3413532e-01, -5.4529995e-01,  3.7609273e-01,  6.4092565e-01,\n",
       "       -9.6930392e-02,  1.8725279e-01,  4.0147740e-01, -6.9192892e-01,\n",
       "       -5.8799241e-02, -6.0994923e-01,  4.3505424e-01, -4.5756572e-01,\n",
       "       -2.3932429e-01,  4.2633045e-02, -9.1068363e-01,  1.9875506e-03,\n",
       "        1.9594520e-01, -2.3439714e-01, -4.7785568e-01,  3.8290903e-01,\n",
       "       -3.4983769e-01,  5.6701694e-02,  4.4497427e-01, -2.0619404e-01,\n",
       "        1.2257517e-01, -3.3563924e-01,  9.9951148e-02, -1.3371135e-01,\n",
       "       -8.0689830e-01, -4.8932323e-01,  2.1899020e-02,  5.4153287e-01,\n",
       "        3.2389438e-01,  5.1791942e-01, -7.3140967e-01, -9.9493355e-01,\n",
       "       -1.4607845e-01,  2.3833303e-01,  4.9303862e-01,  2.4625802e-01,\n",
       "       -2.4954686e-01,  3.4761345e-01, -5.5351353e-01,  1.9754267e-01,\n",
       "       -1.0572939e+00,  5.0580645e-01, -2.4319036e-01, -7.6946771e-01,\n",
       "        5.9507596e-01,  4.8508611e-01, -2.0938461e-01, -9.5754945e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_word_vector = model.wv['food']\n",
    "my_word_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, numpy.ndarray, (200,))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_word_vector), type(my_word_vector), my_word_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.0487982e-01,  7.1753567e-01,  1.6437873e-01, -4.9533224e-04,\n",
       "        -4.0243381e-01,  2.4130361e-01, -1.7375448e-01,  1.9668108e-01,\n",
       "         5.1833510e-01,  7.2512947e-02, -3.7711841e-01, -1.3450913e-01,\n",
       "        -6.1501622e-01,  4.2265847e-02,  1.0030919e+00, -2.6114342e-01,\n",
       "         2.4443835e-02,  5.6554186e-01,  3.3014292e-01,  3.7453169e-01,\n",
       "         2.4988325e-01, -8.7458968e-02,  6.3764280e-01, -4.2551327e-01,\n",
       "         7.8718758e-01, -3.1817943e-01,  1.2278194e-01, -1.0029565e+00,\n",
       "         3.8749295e-01, -2.3035859e-01, -4.2597029e-01, -3.7988597e-01,\n",
       "         4.9015749e-01, -6.0321999e-01, -3.7664428e-01,  3.8577858e-01,\n",
       "        -1.6822539e-01,  5.6258911e-01, -4.2373922e-01, -1.2999626e-01,\n",
       "        -2.8590664e-01,  5.5313420e-01, -2.2693746e-01, -1.3608521e-01,\n",
       "        -6.4299113e-01,  2.0960350e-01, -1.3274111e-01, -4.9843445e-01,\n",
       "        -1.3675939e-01,  2.6674110e-01,  4.2237278e-02,  4.3394662e-02,\n",
       "        -5.5560046e-01, -1.2632187e-01, -4.4564524e-01, -4.6242688e-02,\n",
       "        -2.6184759e-01, -6.8852937e-01, -5.2665526e-01,  7.7356774e-01,\n",
       "        -3.3483304e-02,  1.8740873e-01, -1.4263585e-01, -6.3960135e-01,\n",
       "         3.9438310e-01, -3.8043779e-01,  7.5049222e-01, -2.4091607e-02,\n",
       "         8.3235544e-01, -2.5674272e-01, -2.5011212e-01,  5.0268576e-02,\n",
       "         4.7346011e-02,  2.3800444e-02, -3.4015945e-01, -3.5760015e-01,\n",
       "         8.0075061e-01,  5.9596992e-01, -3.4883174e-01, -1.5500300e-01,\n",
       "        -2.6570040e-01,  2.3179603e-01,  5.2945060e-01,  2.7603361e-01,\n",
       "         3.3864799e-01, -4.3362338e-02,  1.2285306e-01,  2.1278363e-01,\n",
       "         2.9150560e-01, -7.3556408e-02, -4.8083112e-02,  2.7180913e-01,\n",
       "         1.8166043e-01, -4.4898430e-01,  1.2330796e-02, -2.1456532e-01,\n",
       "        -6.2308180e-01,  5.4547995e-01, -1.0433824e+00,  5.3930181e-01,\n",
       "         3.6132309e-01,  5.7421857e-01, -3.0320349e-01, -3.5523310e-01,\n",
       "        -4.7917682e-01,  1.9258200e-01,  2.9334331e-01,  1.3972831e-01,\n",
       "         3.7344680e-03,  4.1523373e-01, -3.7011206e-01,  7.1002573e-01,\n",
       "        -4.7141859e-01,  5.1555139e-01, -2.9442100e-02, -3.7300938e-01,\n",
       "         1.6827925e-01, -7.6328069e-02,  2.6185505e-02,  4.2196313e-01,\n",
       "        -4.6589679e-01, -2.6997393e-01, -4.6027943e-01, -8.4668338e-01,\n",
       "         1.8270352e-01,  9.3102634e-02, -5.5861503e-02, -2.5281957e-01,\n",
       "         2.1937101e-01, -5.6224519e-01,  9.4773926e-02, -1.0958688e-01,\n",
       "        -2.8214762e-01, -2.0505944e-01,  6.4437225e-02,  5.5346394e-01,\n",
       "        -8.0563314e-02, -2.7184871e-01,  5.0157082e-01,  4.4384193e-01,\n",
       "        -4.6333316e-01, -3.9234310e-01, -5.8771305e-02, -7.1571881e-01,\n",
       "         7.4457191e-02, -4.1703045e-02,  2.1811960e-02,  1.2195249e-01,\n",
       "        -9.3413532e-01, -5.4529995e-01,  3.7609273e-01,  6.4092565e-01,\n",
       "        -9.6930392e-02,  1.8725279e-01,  4.0147740e-01, -6.9192892e-01,\n",
       "        -5.8799241e-02, -6.0994923e-01,  4.3505424e-01, -4.5756572e-01,\n",
       "        -2.3932429e-01,  4.2633045e-02, -9.1068363e-01,  1.9875506e-03,\n",
       "         1.9594520e-01, -2.3439714e-01, -4.7785568e-01,  3.8290903e-01,\n",
       "        -3.4983769e-01,  5.6701694e-02,  4.4497427e-01, -2.0619404e-01,\n",
       "         1.2257517e-01, -3.3563924e-01,  9.9951148e-02, -1.3371135e-01,\n",
       "        -8.0689830e-01, -4.8932323e-01,  2.1899020e-02,  5.4153287e-01,\n",
       "         3.2389438e-01,  5.1791942e-01, -7.3140967e-01, -9.9493355e-01,\n",
       "        -1.4607845e-01,  2.3833303e-01,  4.9303862e-01,  2.4625802e-01,\n",
       "        -2.4954686e-01,  3.4761345e-01, -5.5351353e-01,  1.9754267e-01,\n",
       "        -1.0572939e+00,  5.0580645e-01, -2.4319036e-01, -7.6946771e-01,\n",
       "         5.9507596e-01,  4.8508611e-01, -2.0938461e-01, -9.5754945e-01]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_word_vector.reshape((1, 200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 6\n",
    "Поскольку наши данные содержат твиты, а не только слова, нам придется придумать способ использовать векторы слов из модели word2vec для создания векторного представления всего твита. Существует простое решение этой проблемы, мы можем просто взять среднее значение всех векторов слов, присутствующих в твите. Длина результирующего вектора будет одинаковой, то есть 200. Мы повторим тот же процесс для всех твитов в наших данных и получим их векторы. Теперь у нас есть 200 функций word2vec для наших данных.\n",
    "\n",
    "Необходимо создать вектор для каждого твита, взяв среднее значение векторов слов, присутствующих в твите. В цикле сделать:  vec += model_w2v[word].reshape((1, size))\n",
    "и поделить финальный вектор на количество слов в твите.\n",
    "На выходе должен получиться wordvec_df.shape = (49159, 200).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>[when, father, is, dysfunctional, and, is, so,...</td>\n",
       "      <td>[father, dysfunct, selfish, drag, kid, dysfunc...</td>\n",
       "      <td>[father, dysfunctional, selfish, drag, kid, dy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>[thanks, for, lyft, credit, can, not, use, cau...</td>\n",
       "      <td>[thank, lyft, credit, use, caus, offer, wheelc...</td>\n",
       "      <td>[thank, lyft, credit, use, cause, offer, wheel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>[bihday, your, majesty]</td>\n",
       "      <td>[bihday, majesti]</td>\n",
       "      <td>[bihday, majesty]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1    0.0   @user when a father is dysfunctional and is s...   \n",
       "1   2    0.0  @user @user thanks for #lyft credit i can't us...   \n",
       "2   3    0.0                                bihday your majesty   \n",
       "\n",
       "                                         tweet_token  \\\n",
       "0  [when, father, is, dysfunctional, and, is, so,...   \n",
       "1  [thanks, for, lyft, credit, can, not, use, cau...   \n",
       "2                            [bihday, your, majesty]   \n",
       "\n",
       "                                       tweet_stemmed  \\\n",
       "0  [father, dysfunct, selfish, drag, kid, dysfunc...   \n",
       "1  [thank, lyft, credit, use, caus, offer, wheelc...   \n",
       "2                                  [bihday, majesti]   \n",
       "\n",
       "                                    tweet_lemmatized  \n",
       "0  [father, dysfunctional, selfish, drag, kid, dy...  \n",
       "1  [thank, lyft, credit, use, cause, offer, wheel...  \n",
       "2                                  [bihday, majesty]  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = np.zeros(200)\n",
    "    words_counter = 0\n",
    "    \n",
    "    for word in tweet:\n",
    "        try:\n",
    "            vector += modelW2V.wv[word]\n",
    "            words_counter += 1\n",
    "        except:\n",
    "            pass\n",
    "    return (vector / words_counter).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/artem/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "size = 200\n",
    "tweet_averages = []\n",
    "for idx, row in enumerate(df['tweet_token'][:]):\n",
    "    average_per_tweet = None\n",
    "    vec = np.zeros(200)\n",
    "    counter = 0\n",
    "    for ind, word in enumerate(row):\n",
    "        #print(idx, word)\n",
    "        try:\n",
    "            word_vector = model.wv[word]#.reshape((1, 200)) \n",
    "            vec = vec + word_vector\n",
    "            counter = counter +1\n",
    "            #print(idx, word, word_vector)\n",
    "        except:\n",
    "            pass\n",
    "    average_per_tweet = (vec/counter)\n",
    "    tweet_averages.append(average_per_tweet)\n",
    "    #print(average_per_tweet)\n",
    "            \n",
    "#         vec += model.wv[word].reshape((1, size)) \n",
    "#     average_per_tweet = vec / (ind+1)\n",
    "#     print(average_per_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.157515</td>\n",
       "      <td>0.206160</td>\n",
       "      <td>0.240572</td>\n",
       "      <td>0.095627</td>\n",
       "      <td>0.342026</td>\n",
       "      <td>-0.164991</td>\n",
       "      <td>-0.195331</td>\n",
       "      <td>0.147425</td>\n",
       "      <td>0.130821</td>\n",
       "      <td>0.044960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007398</td>\n",
       "      <td>0.013640</td>\n",
       "      <td>-0.110190</td>\n",
       "      <td>0.177521</td>\n",
       "      <td>0.106337</td>\n",
       "      <td>-0.387834</td>\n",
       "      <td>0.149893</td>\n",
       "      <td>0.411490</td>\n",
       "      <td>-0.086332</td>\n",
       "      <td>-0.267600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.099679</td>\n",
       "      <td>0.222602</td>\n",
       "      <td>0.114998</td>\n",
       "      <td>0.011011</td>\n",
       "      <td>0.188402</td>\n",
       "      <td>-0.048495</td>\n",
       "      <td>-0.142211</td>\n",
       "      <td>0.206281</td>\n",
       "      <td>0.384407</td>\n",
       "      <td>-0.152005</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.124551</td>\n",
       "      <td>0.019045</td>\n",
       "      <td>-0.099375</td>\n",
       "      <td>0.190515</td>\n",
       "      <td>0.068947</td>\n",
       "      <td>-0.198821</td>\n",
       "      <td>0.341550</td>\n",
       "      <td>0.362512</td>\n",
       "      <td>0.152765</td>\n",
       "      <td>-0.090385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.317678</td>\n",
       "      <td>-0.127611</td>\n",
       "      <td>-0.241864</td>\n",
       "      <td>-0.143185</td>\n",
       "      <td>0.222966</td>\n",
       "      <td>-0.178260</td>\n",
       "      <td>-0.156288</td>\n",
       "      <td>0.284328</td>\n",
       "      <td>0.674881</td>\n",
       "      <td>-0.172447</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109332</td>\n",
       "      <td>0.322368</td>\n",
       "      <td>-0.172000</td>\n",
       "      <td>0.188761</td>\n",
       "      <td>-0.075118</td>\n",
       "      <td>-0.536051</td>\n",
       "      <td>0.488682</td>\n",
       "      <td>0.605482</td>\n",
       "      <td>0.223515</td>\n",
       "      <td>-0.074148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.074275</td>\n",
       "      <td>-0.001674</td>\n",
       "      <td>-0.168419</td>\n",
       "      <td>0.101925</td>\n",
       "      <td>0.379402</td>\n",
       "      <td>-0.157882</td>\n",
       "      <td>0.003343</td>\n",
       "      <td>0.165702</td>\n",
       "      <td>0.336220</td>\n",
       "      <td>0.031203</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.130294</td>\n",
       "      <td>-0.164786</td>\n",
       "      <td>-0.157801</td>\n",
       "      <td>0.053534</td>\n",
       "      <td>0.211267</td>\n",
       "      <td>-0.189855</td>\n",
       "      <td>0.141836</td>\n",
       "      <td>0.411199</td>\n",
       "      <td>-0.083954</td>\n",
       "      <td>0.137592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.019279</td>\n",
       "      <td>0.059107</td>\n",
       "      <td>0.133410</td>\n",
       "      <td>0.055024</td>\n",
       "      <td>0.243223</td>\n",
       "      <td>-0.249457</td>\n",
       "      <td>0.039960</td>\n",
       "      <td>0.170736</td>\n",
       "      <td>0.080388</td>\n",
       "      <td>-0.263483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052415</td>\n",
       "      <td>-0.111762</td>\n",
       "      <td>-0.281139</td>\n",
       "      <td>0.215636</td>\n",
       "      <td>-0.068879</td>\n",
       "      <td>-0.310696</td>\n",
       "      <td>-0.189842</td>\n",
       "      <td>0.640223</td>\n",
       "      <td>0.089527</td>\n",
       "      <td>-0.304095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49154</th>\n",
       "      <td>0.382059</td>\n",
       "      <td>0.380581</td>\n",
       "      <td>0.068596</td>\n",
       "      <td>-0.058928</td>\n",
       "      <td>0.104340</td>\n",
       "      <td>-0.233776</td>\n",
       "      <td>-0.315963</td>\n",
       "      <td>0.014744</td>\n",
       "      <td>0.138054</td>\n",
       "      <td>0.188783</td>\n",
       "      <td>...</td>\n",
       "      <td>0.187803</td>\n",
       "      <td>-0.106518</td>\n",
       "      <td>0.306282</td>\n",
       "      <td>-0.016223</td>\n",
       "      <td>0.282827</td>\n",
       "      <td>-0.453220</td>\n",
       "      <td>0.122283</td>\n",
       "      <td>0.521535</td>\n",
       "      <td>0.264606</td>\n",
       "      <td>-0.245570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49155</th>\n",
       "      <td>0.114690</td>\n",
       "      <td>0.279147</td>\n",
       "      <td>0.175439</td>\n",
       "      <td>0.014056</td>\n",
       "      <td>0.315212</td>\n",
       "      <td>-0.109102</td>\n",
       "      <td>-0.251520</td>\n",
       "      <td>-0.132618</td>\n",
       "      <td>0.175838</td>\n",
       "      <td>-0.055309</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.165045</td>\n",
       "      <td>-0.095527</td>\n",
       "      <td>-0.067403</td>\n",
       "      <td>0.162235</td>\n",
       "      <td>-0.084300</td>\n",
       "      <td>-0.387289</td>\n",
       "      <td>0.282083</td>\n",
       "      <td>0.477061</td>\n",
       "      <td>0.120090</td>\n",
       "      <td>-0.222803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49156</th>\n",
       "      <td>0.418465</td>\n",
       "      <td>0.085668</td>\n",
       "      <td>0.015495</td>\n",
       "      <td>0.090045</td>\n",
       "      <td>0.146709</td>\n",
       "      <td>-0.118686</td>\n",
       "      <td>-0.022246</td>\n",
       "      <td>0.075554</td>\n",
       "      <td>0.164491</td>\n",
       "      <td>-0.151903</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.186417</td>\n",
       "      <td>0.145597</td>\n",
       "      <td>-0.045228</td>\n",
       "      <td>0.090908</td>\n",
       "      <td>0.040118</td>\n",
       "      <td>-0.358991</td>\n",
       "      <td>0.020523</td>\n",
       "      <td>0.217143</td>\n",
       "      <td>0.104946</td>\n",
       "      <td>-0.163964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49157</th>\n",
       "      <td>0.058645</td>\n",
       "      <td>-0.120679</td>\n",
       "      <td>-0.185780</td>\n",
       "      <td>0.179037</td>\n",
       "      <td>0.041425</td>\n",
       "      <td>-0.222860</td>\n",
       "      <td>0.030920</td>\n",
       "      <td>-0.160227</td>\n",
       "      <td>0.062575</td>\n",
       "      <td>-0.027752</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.126250</td>\n",
       "      <td>-0.284861</td>\n",
       "      <td>-0.148850</td>\n",
       "      <td>0.272023</td>\n",
       "      <td>-0.208541</td>\n",
       "      <td>-0.299925</td>\n",
       "      <td>0.591013</td>\n",
       "      <td>0.718859</td>\n",
       "      <td>0.226171</td>\n",
       "      <td>-0.334430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49158</th>\n",
       "      <td>0.031096</td>\n",
       "      <td>0.189441</td>\n",
       "      <td>-0.091066</td>\n",
       "      <td>0.126903</td>\n",
       "      <td>0.518127</td>\n",
       "      <td>-0.123786</td>\n",
       "      <td>-0.084454</td>\n",
       "      <td>-0.195277</td>\n",
       "      <td>0.288815</td>\n",
       "      <td>-0.210743</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.106324</td>\n",
       "      <td>0.162295</td>\n",
       "      <td>-0.159796</td>\n",
       "      <td>0.180171</td>\n",
       "      <td>-0.177308</td>\n",
       "      <td>-0.405516</td>\n",
       "      <td>0.226966</td>\n",
       "      <td>0.556915</td>\n",
       "      <td>0.435394</td>\n",
       "      <td>-0.206793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49159 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "0      0.157515  0.206160  0.240572  0.095627  0.342026 -0.164991 -0.195331   \n",
       "1      0.099679  0.222602  0.114998  0.011011  0.188402 -0.048495 -0.142211   \n",
       "2      0.317678 -0.127611 -0.241864 -0.143185  0.222966 -0.178260 -0.156288   \n",
       "3      0.074275 -0.001674 -0.168419  0.101925  0.379402 -0.157882  0.003343   \n",
       "4      0.019279  0.059107  0.133410  0.055024  0.243223 -0.249457  0.039960   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "49154  0.382059  0.380581  0.068596 -0.058928  0.104340 -0.233776 -0.315963   \n",
       "49155  0.114690  0.279147  0.175439  0.014056  0.315212 -0.109102 -0.251520   \n",
       "49156  0.418465  0.085668  0.015495  0.090045  0.146709 -0.118686 -0.022246   \n",
       "49157  0.058645 -0.120679 -0.185780  0.179037  0.041425 -0.222860  0.030920   \n",
       "49158  0.031096  0.189441 -0.091066  0.126903  0.518127 -0.123786 -0.084454   \n",
       "\n",
       "            7         8         9    ...       190       191       192  \\\n",
       "0      0.147425  0.130821  0.044960  ... -0.007398  0.013640 -0.110190   \n",
       "1      0.206281  0.384407 -0.152005  ... -0.124551  0.019045 -0.099375   \n",
       "2      0.284328  0.674881 -0.172447  ...  0.109332  0.322368 -0.172000   \n",
       "3      0.165702  0.336220  0.031203  ... -0.130294 -0.164786 -0.157801   \n",
       "4      0.170736  0.080388 -0.263483  ...  0.052415 -0.111762 -0.281139   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "49154  0.014744  0.138054  0.188783  ...  0.187803 -0.106518  0.306282   \n",
       "49155 -0.132618  0.175838 -0.055309  ... -0.165045 -0.095527 -0.067403   \n",
       "49156  0.075554  0.164491 -0.151903  ... -0.186417  0.145597 -0.045228   \n",
       "49157 -0.160227  0.062575 -0.027752  ... -0.126250 -0.284861 -0.148850   \n",
       "49158 -0.195277  0.288815 -0.210743  ... -0.106324  0.162295 -0.159796   \n",
       "\n",
       "            193       194       195       196       197       198       199  \n",
       "0      0.177521  0.106337 -0.387834  0.149893  0.411490 -0.086332 -0.267600  \n",
       "1      0.190515  0.068947 -0.198821  0.341550  0.362512  0.152765 -0.090385  \n",
       "2      0.188761 -0.075118 -0.536051  0.488682  0.605482  0.223515 -0.074148  \n",
       "3      0.053534  0.211267 -0.189855  0.141836  0.411199 -0.083954  0.137592  \n",
       "4      0.215636 -0.068879 -0.310696 -0.189842  0.640223  0.089527 -0.304095  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "49154 -0.016223  0.282827 -0.453220  0.122283  0.521535  0.264606 -0.245570  \n",
       "49155  0.162235 -0.084300 -0.387289  0.282083  0.477061  0.120090 -0.222803  \n",
       "49156  0.090908  0.040118 -0.358991  0.020523  0.217143  0.104946 -0.163964  \n",
       "49157  0.272023 -0.208541 -0.299925  0.591013  0.718859  0.226171 -0.334430  \n",
       "49158  0.180171 -0.177308 -0.405516  0.226966  0.556915  0.435394 -0.206793  \n",
       "\n",
       "[49159 rows x 200 columns]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec_df = pd.DataFrame(tweet_averages)\n",
    "wordvec_df.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
